{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer-graded Assignment: Эксперименты с моделью\n",
    "\n",
    "На прошлой неделе вы поучаствовали в соревновании на kaggle и, наверняка, большинство успешно справилось с прохождением baseline, а значит пора двигаться дальше - заняться оптимизацией модели, провести серию экспериментов и построить сильное финальное решения.\n",
    "\n",
    "В этом задании вам нужно провести ряд эскпериментов, оценить качество полученных в процессе экспериментирования моделей и выбрать лучшее решение. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание будет оцениваться на основании загруженного jupyther notebook и развернутых ответов на поставленные вопросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инструкции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Начнем с простого. Давайте оценим как много объектов действительно нужно для построения качественной модели. Для обучения доступна достаточно большая выборка и может так оказаться, что начиная с некоторого момента рост размера обучающей выборки перестает влиять на качество модели. Постройте кривые обучения, обучая модель на выборках разного размера начиная с небольшого количество объектов в обучающей выборке и постепенно наращивая её размер с некоторым шагом. Обратите внимание на `sklearn.model_selection.learning_curve`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала предобработаем данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('orange_small_churn_train_data.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numeric_columns = np.array(train.columns[:190])\n",
    "categorical_columns = np.array(train.columns[190:230])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для построения модели выберем только те переменные, в которых есть хотя бы одно значение, не равное NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_null_numeric_columns = np.array(numeric_columns[np.array(pd.notnull(train[numeric_columns]).any())])\n",
    "not_null_categorical_columns = np.array(categorical_columns[np.array(pd.notnull(train[categorical_columns]).any())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для упрощения модели выберем только те категориальные признаки, в которых не больше ста различных значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_less_or_equal_100 = []\n",
    "for var in not_null_categorical_columns:\n",
    "    if len(train[var].unique()) <= 100:\n",
    "        categorical_less_or_equal_100.append(var)\n",
    "categorical_less_or_equal_100 = np.array(categorical_less_or_equal_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_numeric = train[not_null_numeric_columns]\n",
    "train_categorical = train[categorical_less_or_equal_100]\n",
    "y_train = train['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним пропущенные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_numeric = train_numeric.fillna(0)\n",
    "train_categorical = train_categorical.fillna('NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим к категориальным данным one-hot encoding с помощью DictVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dv = DictVectorizer(sparse = False)\n",
    "train_categorical = pd.DataFrame(dv.fit_transform(train_categorical.T.to_dict().values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat([train_numeric, train_categorical], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьём обучающую выборку на 3 фолда методом stratified k-fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим кривые обучения. В качестве модели выберем градиентный бустинг (xgboost.XGBClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAF5CAYAAACIpbAsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a6e2d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import xgboost as xgb\n",
    "train_sizes_abs, train_scores, test_scores = learning_curve(xgb.XGBClassifier(), X_train, y_train, cv = skf,\n",
    "                                                            scoring = 'roc_auc')\n",
    "train_scores_mean = np.mean(train_scores, axis = 1)\n",
    "test_scores_mean = np.mean(test_scores, axis = 1)\n",
    "plt.figure()\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('Score')\n",
    "plt.grid()\n",
    "plt.plot(train_sizes_abs, train_scores_mean, 'o-', color = 'r', label = 'Training score')\n",
    "plt.plot(train_sizes_abs, test_scores_mean, 'o-', color = 'g', label = 'Cross-validation score')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что с увеличением размера обучающей выборки качество на тестовой выборке продолжает расти. Следовательно, для построения модели нужно использовать всю выборку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Часто несбалансированные по классам выборки приводят к различным проблемам при обучении моделей. Давайте попробуем по-разному обработать выборку, поиграть с распределением объектов по классам и сделать выводы о том, как соотношение классов влияет на качество модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1\\. Задайте веса объектам так, чтобы соотношение классов с учетом весов объектов изменилось. Попробуйте не менее трёх различных вариантов весов. Меняются ли результаты классификации? Как это сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала оценим качество модели без весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Без весов:  0.739371390953\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(xgb.XGBClassifier(), X_train, y_train, scoring = 'roc_auc', cv = skf)\n",
    "print 'Без весов: ', scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем три варианта весов: 1:5, 1:10, 1:13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.737038471712\n",
      "0.735925157592\n",
      "0.7356204807\n"
     ]
    }
   ],
   "source": [
    "sample_weight1 = np.array([1 if x == -1 else 5 for x in y_train])\n",
    "sample_weight2 = np.array([1 if x == -1 else 10 for x in y_train])\n",
    "sample_weight3 = np.array([1 if x == -1 else 13 for x in y_train])\n",
    "for sample_weight in sample_weight1, sample_weight2, sample_weight3:\n",
    "    scores = cross_val_score(xgb.XGBClassifier(), X_train, y_train, scoring = 'roc_auc', cv = skf,\n",
    "                             fit_params = {'sample_weight': sample_weight})\n",
    "    print scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество поменялось. Видим, что наибольшее качество достигается при весах 1:5, однако это всё равно меньше, чем у модели без весов. Поэтому далее веса использовать не будем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2\\. Примените к выборке технологию undersampling: для этого нужно убрать из обучения некоторое количество объектов большего класса таким образом, чтобы соотношение классов изменилось. Попробуйте не менее трёх различных вариантов undersampling (варианты могут отличаться как по количество отфильтрованных объектов, так и по принципу выборка объектов для отсеивания из выборки). Меняются ли результаты классификации? Как это сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем убрать 10000, 20000 и 35000 объектов класса -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 : 0.737970202562\n",
      "20000 : 0.738587331131\n",
      "35000 : 0.722598772375\n"
     ]
    }
   ],
   "source": [
    "indices = np.array(y_train[y_train == -1].index)\n",
    "np.random.seed(0)\n",
    "for size in 10000, 20000, 35000:\n",
    "    indices_to_remove = np.random.choice(indices, size = size, replace = False)\n",
    "    scores = cross_val_score(xgb.XGBClassifier(), X_train.drop(indices_to_remove), y_train.drop(indices_to_remove),\n",
    "                             scoring = 'roc_auc', cv = skf)\n",
    "    print size, ':', scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество поменялось. Видим, что наибольшее качество достигается при удалении 20000 объектов, однако это всё равно меньше, чем у модели без использования undersampling. Поэтому далее undersampling использовать не будем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Теперь перейдем к работе с признаками. Ранее вы реализовали несколько стратегий для обработки пропущенных значений. Сравните эти стратегии между собой с помощью оценки качества моделей кросс-валидации, построенных на датасетах с использованием различных стратегий. Как обработка пропущенных значений сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначально мы заполняли пропущенные значения нулями. Посмотрим на качество модели при такой стратегии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Заполнение нулями:  0.739371390953\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(xgb.XGBClassifier(), X_train, y_train, scoring = 'roc_auc', cv = skf)\n",
    "print 'Заполнение нулями: ', scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем заполнить пропущенные значения средними по столбцам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Заполнение средними:  0.740679192064\n"
     ]
    }
   ],
   "source": [
    "train_numeric_means = train[not_null_numeric_columns].fillna(train[not_null_numeric_columns].mean(0), axis = 0)\n",
    "X_train_means = pd.concat([train_numeric_means, train_categorical], axis = 1)\n",
    "scores = cross_val_score(xgb.XGBClassifier(), X_train_means, y_train, scoring = 'roc_auc', cv = skf)\n",
    "print 'Заполнение средними: ', scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем заполнить пропущенные значения медианами по столбцам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Заполнение медианами:  0.734332618712\n"
     ]
    }
   ],
   "source": [
    "train_numeric_medians = train[not_null_numeric_columns].fillna(train[not_null_numeric_columns].median(0), axis = 0)\n",
    "X_train_medians = pd.concat([train_numeric_medians, train_categorical], axis = 1)\n",
    "scores = cross_val_score(xgb.XGBClassifier(), X_train_medians, y_train, scoring = 'roc_auc', cv = skf)\n",
    "print 'Заполнение медианами: ', scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что лучшее качество достигается при заполнении пропущенных значений средними. Далее будем использовать именно эту стратегию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_numeric = train_numeric_means\n",
    "X_train = X_train_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Также вы уже реализовали несколько стратегий для обработки категориальных признаков. Сравните эти стратегии между собой с помощью оценки качества моделей по кросс-валидации, построенных на датасетах с использованием различных стратегий. Как обработка категориальных признаков сказывается на качестве модели? Какой вариант выглядит наиболее оптимальным с точки зрения качества?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначально для обработки категориальных признаков мы использовали one-hot encoding. Посмотрим на качество модели при такой стратегии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding:  0.740679192064\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(xgb.XGBClassifier(), X_train, y_train, scoring = 'roc_auc', cv = skf)\n",
    "print 'One-hot encoding: ', scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем label encoding: будем кодировать каждую категорию с помощью одного целочисленного значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding:  0.737687905618\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_categorical_le = train[not_null_categorical_columns]\n",
    "train_categorical_le = train_categorical_le.fillna('NA')\n",
    "for var in not_null_categorical_columns:\n",
    "    train_categorical_le[var] = le.fit_transform(train_categorical_le[var])\n",
    "X_train_le = pd.concat([train_numeric, train_categorical_le], axis = 1)\n",
    "scores = cross_val_score(xgb.XGBClassifier(), X_train_le, y_train, scoring = 'roc_auc', cv = skf)\n",
    "print 'Label encoding: ', scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем кодировать частотами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequencies encoding:  0.744447036908\n"
     ]
    }
   ],
   "source": [
    "train_categorical_freq = train[not_null_categorical_columns]\n",
    "train_categorical_freq = train_categorical_freq.fillna('NA')\n",
    "for var in not_null_categorical_columns:\n",
    "    value_counts = train_categorical_freq[var].value_counts()\n",
    "    train_categorical_freq[var] = np.array([value_counts[x] for x in train_categorical_freq[var]])\n",
    "X_train_freq = pd.concat([train_numeric, train_categorical_freq], axis = 1)\n",
    "scores = cross_val_score(xgb.XGBClassifier(), X_train_freq, y_train, scoring = 'roc_auc', cv = skf)\n",
    "print 'Frequencies encoding: ', scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что лучшее качество достигается при кодировании категориальных признаков частотами. Далее будем использовать именно эту стратегию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_categorical = train_categorical_freq\n",
    "X_train = X_train_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Все ли признаки оказались полезными для построения моделей? Проведите процедуру отбора признаков, попробуйте разные варианты отбора (обратите внимание на модуль `sklearn.feature_selection`). Например, можно выбрасывать случайные признаки или строить отбор на основе l1-регуляризации - отфильтровать из обучения признаки, которые получат нулевой вес при построении регрессии с l1-регуляризацией (`sklearn.linear_model.Lasso`). И всегда можно придумать что-то своё=) Попробуйте как минимум 2 различные стратегии, сравните результаты. Помог ли отбор признаков улучшить качество модели? Поясните свой ответ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала удалим признаки с нулевой дисперсией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X_train_vt = VarianceThreshold().fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем отобрать 40 лучших признаков, используя ANOVA F-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA F-value:  0.74588496742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "X_f_classif = SelectKBest(k = 40).fit_transform(X_train_vt, y_train)\n",
    "scores = cross_val_score(xgb.XGBClassifier(), X_f_classif, y_train, scoring = 'roc_auc', cv = skf)\n",
    "print 'ANOVA F-value: ', scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество чуть улучшилось. Теперь попробуем построить регрессию Lasso и убрать признаки с нулевыми весами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso:  0.688986238729\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(max_iter = 10000).fit(X_train_vt, y_train)\n",
    "nonzero_features = np.flatnonzero(lasso.coef_)\n",
    "X_train_lasso = X_train_vt.take(nonzero_features, axis = 1)\n",
    "scores = cross_val_score(xgb.XGBClassifier(), X_train_lasso, y_train, scoring = 'roc_auc', cv = skf)\n",
    "print 'Lasso: ', scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество сильно ухудшилось. Далее будем использовать первую стратегию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Подберите оптимальные параметры модели. Обратите внимание, что в зависимости от того, как вы обработали исходные данные, сделали ли балансировку классов, сколько объектов оставили в обучающей выборке и др. оптимальные значения параметров могут меняться. Возьмите наилучшее из ваших решений на текущий момент и проведите процедуру подбора параметров модели (обратите внимание на `sklearn.model_selection.GridSearchCV`) Как подбор параметров повлиял на качество модели?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем подбирать параметры max_depth и learning_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'learning_rate': 0.1, 'max_depth': 3}\n",
      "Best score:  0.74588499581\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'max_depth' : [3, 6, 10], 'learning_rate' : [0.01, 0.05, 0.1, 0.2]}\n",
    "search = GridSearchCV(xgb.XGBClassifier(), param_grid, scoring = 'roc_auc', cv = skf)\n",
    "search.fit(X_f_classif, y_train)\n",
    "print 'Best params: ', search.best_params_\n",
    "print 'Best score: ', search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что оптимальными параметрами являются learning_rate = 0.1 и max_depth = 3. Однако качество у нас практически не изменилось, так как эти же параметры являются в XGBClassifier параметрами по умолчанию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Предложите методику оценки того, какие признаки внесли наибольший вклад в модель (например, это могут быть веса в случае регрессии, а также большое количество моделей реализуют метод `feature_importances_` - оценка важности признаков). На основе предложенной методики проанализируйте, какие признаки внесли больший вклад в модель, а какие меньший?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсортируем признаки по feature\\_importances_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Var126</td>\n",
       "      <td>0.144105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Var217</td>\n",
       "      <td>0.069869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Var189</td>\n",
       "      <td>0.059680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Var199</td>\n",
       "      <td>0.058224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Var192</td>\n",
       "      <td>0.053857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Var113</td>\n",
       "      <td>0.046579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Var73</td>\n",
       "      <td>0.046579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Var81</td>\n",
       "      <td>0.040757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Var74</td>\n",
       "      <td>0.034934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Var57</td>\n",
       "      <td>0.032023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Var28</td>\n",
       "      <td>0.021834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Var153</td>\n",
       "      <td>0.018923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Var226</td>\n",
       "      <td>0.017467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Var212</td>\n",
       "      <td>0.016012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Var168</td>\n",
       "      <td>0.014556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Var197</td>\n",
       "      <td>0.014556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Var140</td>\n",
       "      <td>0.014556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Var205</td>\n",
       "      <td>0.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Var218</td>\n",
       "      <td>0.013100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Var45</td>\n",
       "      <td>0.011645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Var210</td>\n",
       "      <td>0.011645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Var13</td>\n",
       "      <td>0.011645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Var202</td>\n",
       "      <td>0.010189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Var198</td>\n",
       "      <td>0.010189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Var53</td>\n",
       "      <td>0.008734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Var134</td>\n",
       "      <td>0.008734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Var133</td>\n",
       "      <td>0.008734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Var125</td>\n",
       "      <td>0.008734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Var206</td>\n",
       "      <td>0.008734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Var38</td>\n",
       "      <td>0.008734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Var122</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Var124</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Var127</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Var128</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Var129</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Var130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Var131</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Var135</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Var106</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Var104</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Var80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Var103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Var82</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Var84</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Var86</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Var87</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Var88</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Var89</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Var90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Var91</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Var92</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Var93</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Var95</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Var96</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Var97</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Var98</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Var99</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Var100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Var101</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Var118</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature  feature_importance\n",
       "114  Var126            0.144105\n",
       "199  Var217            0.069869\n",
       "172  Var189            0.059680\n",
       "182  Var199            0.058224\n",
       "175  Var192            0.053857\n",
       "101  Var113            0.046579\n",
       "62    Var73            0.046579\n",
       "69    Var81            0.040757\n",
       "63    Var74            0.034934\n",
       "46    Var57            0.032023\n",
       "24    Var28            0.021834\n",
       "140  Var153            0.018923\n",
       "208  Var226            0.017467\n",
       "194  Var212            0.016012\n",
       "154  Var168            0.014556\n",
       "180  Var197            0.014556\n",
       "128  Var140            0.014556\n",
       "188  Var205            0.013100\n",
       "200  Var218            0.013100\n",
       "37    Var45            0.011645\n",
       "192  Var210            0.011645\n",
       "11    Var13            0.011645\n",
       "185  Var202            0.010189\n",
       "181  Var198            0.010189\n",
       "43    Var53            0.008734\n",
       "122  Var134            0.008734\n",
       "121  Var133            0.008734\n",
       "113  Var125            0.008734\n",
       "189  Var206            0.008734\n",
       "32    Var38            0.008734\n",
       "..      ...                 ...\n",
       "110  Var122            0.000000\n",
       "112  Var124            0.000000\n",
       "115  Var127            0.000000\n",
       "116  Var128            0.000000\n",
       "117  Var129            0.000000\n",
       "118  Var130            0.000000\n",
       "119  Var131            0.000000\n",
       "123  Var135            0.000000\n",
       "94   Var106            0.000000\n",
       "92   Var104            0.000000\n",
       "68    Var80            0.000000\n",
       "91   Var103            0.000000\n",
       "70    Var82            0.000000\n",
       "72    Var84            0.000000\n",
       "74    Var86            0.000000\n",
       "75    Var87            0.000000\n",
       "76    Var88            0.000000\n",
       "77    Var89            0.000000\n",
       "78    Var90            0.000000\n",
       "79    Var91            0.000000\n",
       "80    Var92            0.000000\n",
       "81    Var93            0.000000\n",
       "83    Var95            0.000000\n",
       "84    Var96            0.000000\n",
       "85    Var97            0.000000\n",
       "86    Var98            0.000000\n",
       "87    Var99            0.000000\n",
       "88   Var100            0.000000\n",
       "89   Var101            0.000000\n",
       "106  Var118            0.000000\n",
       "\n",
       "[212 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier()\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "features = pd.DataFrame({'feature' : X_train.columns, 'feature_importance' : xgb_clf.feature_importances_})\n",
    "features.sort_values('feature_importance', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что наибольший вклад в модель внесли признаки Var126, Var217, Var189, Var199, Var192. Наименьший вклад в модель внесли признаки, находящиеся внизу выборки, такие как Var122, Var124, Var127 и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Напоследок давайте посмотрим на объекты. На каких объектах достигается наибольшая ошибка классификации? Есть ли межу этими объектами что-то общее? Видны ли какие-либо закономерности? Предположите, почему наибольшая ошибка достигается именно на этих объектах. В данном случае \"наибольшую\" ошибку можно понимать как отнесение объекта с чужому классу с большой долей уверенности (с высокой вероятностью)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier()\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "predictions = xgb_clf.predict(X_train)\n",
    "probabilities = [x[1] for x in xgb_clf.predict_proba(X_train)]\n",
    "false_positives = X_train[np.array(y_train == -1 & np.logical_and(predictions == 1, probabilities >= 0.9))]\n",
    "false_negatives = X_train[np.array(y_train == 1 & np.logical_and(predictions == -1, probabilities <= 0.1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на объекты, с высокой вероятностью отнесённые к классу 1, хотя принадлежащие классу -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.16200000e+03,   7.00000000e+00,   2.88000000e+02, ...,\n",
       "          2.81120000e+04,   2.62110000e+04,   2.27770000e+04],\n",
       "       [  1.12000000e+02,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.81120000e+04,   2.62110000e+04,   2.27770000e+04],\n",
       "       [  7.70000000e+01,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.81120000e+04,   2.62110000e+04,   2.27770000e+04],\n",
       "       ..., \n",
       "       [  7.98000000e+02,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.81120000e+04,   2.62110000e+04,   2.27770000e+04],\n",
       "       [  8.05000000e+02,   7.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.81120000e+04,   2.62110000e+04,   2.27770000e+04],\n",
       "       [  5.04000000e+02,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.81120000e+04,   2.62110000e+04,   2.27770000e+04]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что на первый взгляд они довольно похожи. Посмотрим на объекты в выборке, действительно принадлежащие классу 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.53300000e+03,   7.00000000e+00,   4.00000000e+00, ...,\n",
       "          2.81120000e+04,   2.62110000e+04,   2.27770000e+04],\n",
       "       [  2.52000000e+02,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          2.81120000e+04,   2.62110000e+04,   2.27770000e+04],\n",
       "       [  5.85900000e+03,   7.00000000e+00,   1.04000000e+02, ...,\n",
       "          2.81120000e+04,   2.62110000e+04,   2.27770000e+04],\n",
       "       ..., \n",
       "       [  2.31000000e+02,   0.00000000e+00,   1.60000000e+01, ...,\n",
       "          2.81120000e+04,   2.62110000e+04,   7.85000000e+03],\n",
       "       [  1.27400000e+03,   7.00000000e+00,   5.96000000e+02, ...,\n",
       "          2.81120000e+04,   2.62110000e+04,   7.85000000e+03],\n",
       "       [  9.87000000e+02,   7.00000000e+00,   1.47600000e+03, ...,\n",
       "          2.81120000e+04,   2.62110000e+04,   2.27770000e+04]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[np.array(y_train == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что они также похожи на false_positives. Возможная причина таких ошибок - похожесть объектов false_positives на объекты класса 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на объекты, с высокой вероятностью отнесённые к классу -1, хотя принадлежащие классу 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 40), dtype=float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таких объектов нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. По итогам проведенных экспериментов постройте финальную решение - модель с наилучшим качеством. Укажите, какие преобразования данных, параметры и пр. вы выбрали для построения финальной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель уже построена. Я использовала заполнение пропущенных значений средними, кодирование категориальных признаков частотами, выбрала 40 наиболее важных переменных с помощью ANOVA F-value. Все параметры модели я использовала по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier()\n",
    "xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Подумайте, можно ли еще улучшить модель? Что для этого можно сделать? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Можно добавить новые переменные - полиномиальные или индикаторы того, пропущено значение или нет. Можно отмасштабировать вещественные признаки. Также можно попробовать подобрать другие параметры модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
